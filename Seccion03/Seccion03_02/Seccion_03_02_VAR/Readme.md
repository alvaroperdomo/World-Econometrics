# ¬øQu√© es un modelo VAR?

Considere el sistema bivariado simple[^1]:

[^1]: **Por razones pedag√≥gicas nos centraremos principalemente en el an√°lisis de este sistema _VAR_ dos variables. Este _VAR_ sencillo es √∫til para ilustrar los sistemas multivariados de orden superior que se presentaran m√°s adelante**

$I$) $y_t=b_{10}-b_{12}z_t+\gamma_{11}y_{t-1}+\gamma_{12}z_{t-1}+\varepsilon_{yt}$  

$II$) $z_t=b_{20}-b_{21}y_t+\gamma_{21}y_{t-1}+\gamma_{22}z_{t-1}+\varepsilon_{zt}$   

donde se supone que 
* tanto { $y_t$ } como { $z_t$ } son series estacionarias; 
* { $\varepsilon_{yt}$ }  y { $\varepsilon_{zt}$ } son perturbaciones ruido blanco con desviaciones est√°ndar $\sigma_y$  y $\sigma_z$, respectivamente; y 
* { $\varepsilon_{yt}$ } y { $\varepsilon_{zt}$ } son perturbaciones ruido blanco no correlacionadas.

Las ecuaciones $I$ y $II$ constituyen un vector autorregresivo de primer orden $VAR(1)$ porque la longitud de rezago m√°s larga dentro del sistema es $1$. La estructura del sistema incorpora retroalimentaci√≥n porque permite que $y_t$ y $z_t$ se afecten entre s√≠. Por ejemplo, $-b_{12}$  es el efecto contempor√°neo de un cambio unitario de $z_t$ en $y_t$ y $\gamma_{12}$ es el efecto de un cambio de unitario de $z_{t-1}$ en $y_t$. 

Dado que los t√©rminos $\varepsilon_{yt}$ y $\varepsilon_{zt}$ son innovaciones puras (o choques) en $y_t$ y $z_t$, respectivamente. Entonces, 
* si $b_{21}‚â†0$, $\varepsilon_{yt}$  tiene un efecto contempor√°neo indirecto en $z_t$, y
* si $b_{12}‚â†0$, $\varepsilon_{zt}$  tiene un efecto contempor√°neo indirecto en $y_t$. 

Las ecuaciones $I$ y $II$ no pueden ser estimadas por M√≠nimos Cuadrados Ordinarios ( $MCO$ ) ya que 
* $y_t$ tiene un efecto contempor√°neo en $z_t$ y
* $z_t$ tiene un efecto contempor√°neo en $y_t$.

Por lo tanto, las estimaciones de $MCO$ sufrir√≠an un sesgo de simultaneidad en donde los regresores y los t√©rminos de error estar√≠an correlacionados.

Es posible transformar el sistema de ecuaciones en una forma compacta: $\mathbf{B x_t= \Gamma_0 + \Gamma_1 x_{t-1}+\varepsilon_t}$

donde $\eqalign{\mathbf{B} = {\left\lbrack \matrix{1 & b_{12} \cr b_{21} & 1} \right\rbrack}}$, $\eqalign{\mathbf{x_t} = {\left\lbrack \matrix{y_t \cr z_t} \right\rbrack}}$, $\eqalign{\mathbf{\Gamma_0}= {\left\lbrack \matrix{b_{10} \cr b_{20}} \right\rbrack}}$, $\eqalign{\mathbf{\Gamma_1} = {\left\lbrack \matrix{\gamma_{11} & \gamma_{12} \cr \gamma_{21} & \gamma_{22}} \right\rbrack}}$ y $\eqalign{\mathbf{\varepsilon_t} = {\left\lbrack \matrix{\varepsilon_{yt} \cr \varepsilon_{zt}} \right\rbrack}}$

Al premultiplicar por $\mathbf{B^{‚àí1}}$ se obtiene $\mathbf{x_t= A_0 + A_1x_{t-1}+e_t}$  donde $\mathbf{A_0=B^{‚àí1}\Gamma_0}$, $\mathbf{A_1=B^{‚àí1}\Gamma_1}$, y $\mathbf{e_t=B^{‚àí1}\varepsilon_t}$

Para prop√≥sitos de notaci√≥n, podemos definir
* $a_{i0}$ como el elemento $i$ del vector $\mathbf{A_0}$,
* $a_{ij}$ como el elemento que se encuentra en la fila $i$ y en la columna $j$ de la matriz $\mathbf{A_1}$, y
* $e_{it}$ como el elemento $i$ del vector $\mathbf{e_t}$. 

Usando esta nueva notaci√≥n, podemos reescribir $\mathbf{x_t= A_0 + A_1x_{t-1}+e_t}$ en la forma equivalente:

$i$) $y_t=a_{10}+a_{11}y_{t-1}+a_{12}z_{t-1}+e_{1t}$ 

$ii$) $z_t=a_{20}+a_{21}y_{t-1}+a_{22}z_{t-1}+e_{2t}$

Para distinguir entre los sistemas representados por $I$ y $II$ versus $i$ y $ii$, el primero se llama un **VAR estructural o sistema primitivo** y el segundo se llama un **VAR en forma est√°ndar**. 

Es importante tener en cuenta que los t√©rminos de error (es decir, $e_{1t}$ y $e_{2t}$) son compuestos de los dos choques $\varepsilon_{yt}$  y $\varepsilon_{zt}$. Y se puede demostrar, aunque aqu√≠ no lo desarrollaremos, que dado que como $\varepsilon_{yt}$  y $\varepsilon_{zt}$ son procesos de ruido blanco, entonces $e_{1t}$ y $e_{2t}$ son ruido blanco

Un objetivo expl√≠cito del enfoque de Box-Jenkins es proporcionar una metodolog√≠a que conduzca a modelos parsimoniosos. El objetivo final de hacer pron√≥sticos precisos a corto plazo se logra mejor eliminando las estimaciones de par√°metros insignificantes del modelo. Sin embargo, Sims (1980) aboga por una estrategia de estimaci√≥n alternativa. Considere la siguiente generalizaci√≥n multivariada de un proceso autorregresivo $\mathbf{x_t=A_0 +  A_1x_{t-1} + \dots + A_p x_{t-p} + e_t}$ donde 
* $\mathbf{x_t}$ es un vector ($n \times 1$) que re√∫ne las ùëõ variables incluidas en el $VAR$
* $\mathbf{A_0}$ es un vector ($n \times 1$) de interceptos
* $\mathbf{A_i}$ son las matrices ($n \times n$) de coeficientes
* $\mathbf{e_t}$ es un vector ($n \times 1$) de los t√©rminos de error

La metodolog√≠a de Sims implica √∫nicamente:
1) la determinaci√≥n de las variables apropiadas para incluir en el $VAR$: Las variables que se incluir√°n en el $VAR$ se seleccionan de acuerdo con el modelo econ√≥mico relevante.
2) la determinaci√≥n de la longitud de rezagos apropiada: Existen una serie de pruebas (las cuales se explicar√°n m√°s adelante) para hacer esto. No se hace un intento expl√≠cito de "reducir" el n√∫mero de par√°metros estimados. La matriz $\mathbf{A_0}$ contiene $n$ par√°metros, y cada matriz $\mathbf{A_i}$ contiene $n^2$ par√°metros; por lo tanto, $n+pn^2$ coeficientes deben ser estimados. 

Sin lugar a dudas, el $VAR$ esta sobreparameterizado ya que muchas de las estimaciones de los coeficientes ser√°n no significativas. Sin embargo, el objetivo es encontrar las interrelaciones importantes entre las variables. Imponer incorrectamente restricciones nulas puede desperdiciar informaci√≥n importante. Adem√°s, es probable que los regresores sean altamente colineales, de modo que las pruebas $t$ sobre los coeficientes individuales no son gu√≠as confiables para reducir el modelo.

De todas formas, 

## Pron√≥sticos
Una vez que se ha estimado el $VAR$, se puede utilizar como un modelo de pron√≥stico de m√∫ltiples ecuaciones. 

Suponga que estima el modelo de primer orden $\mathbf{x_t=A_0+A_1x_{t-1}+e_t}$ para obtener los valores de los coeficientes en $\mathbf{A_0}$ y en $\mathbf{A_1}$. Si sus datos van hasta el per√≠odo $T$, es sencillo obtener los pron√≥sticos de un periodo hacia adelante de sus variables utilizando la relaci√≥n $E_T\mathbf{x_{T+1}=A_0+A_1x_T}$. De manera similar, se puede obtener un pron√≥stico de dos periodos hacia adelante utilizando la relaci√≥n $E_T\mathbf{x_{T+2}=A_0+A_1x_{T+1}=A_0+A_1(A_0+A_1x_T)}$. 

Sin embargo, en un modelo de orden superior, puede haber un gran n√∫mero de coeficientes estimados. Dado que los ùëâùê¥ùëÖ no restringidos est√°n sobreparametrizados, los pron√≥sticos pueden ser poco confiables. Con el fin de obtener un modelo parsimonioso, muchos pronosticadores eliminar√≠an los coeficientes no significativos del $VAR$. Para despu√©s volver a estimar el modelo llamado $near-VAR$ mediante el uso de un $SUR$, para utilizar con fines de pron√≥stico. 
